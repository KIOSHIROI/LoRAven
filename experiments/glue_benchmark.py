"""
GLUEåŸºå‡†æµ‹è¯•ï¼šLoRAã€AdaLoRAã€DoRAã€LoRAvenå¯¹æ¯”å®éªŒ
æ”¯æŒSST-2ã€MNLIã€QNLIã€RTEã€CoLAä»»åŠ¡çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ
"""

import os
# è®¾ç½®HFé•œåƒç«™è§£å†³è¿æ¥é—®é¢˜
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# è®¾ç½®å¤šGPUå¹¶è¡Œè®­ç»ƒï¼Œä½¿ç”¨cuda:0, cuda:1, cuda:3
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"

# è®¾ç½®tokenizerså¹¶è¡ŒåŒ–ï¼Œé¿å…forkè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# è®¾ç½®Hugging Faceç¼“å­˜ç›®å½•åˆ°é¡¹ç›®ç›®å½•ï¼Œé¿å…åœ¨/homeç›®å½•åˆ›å»ºå†…å®¹
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
hf_cache_dir = os.path.join(project_root, ".hf_cache")
os.environ["HF_HOME"] = hf_cache_dir
os.environ["TRANSFORMERS_CACHE"] = os.path.join(hf_cache_dir, "transformers")
os.environ["HF_DATASETS_CACHE"] = os.path.join(hf_cache_dir, "datasets")
os.environ["HUGGINGFACE_HUB_CACHE"] = os.path.join(hf_cache_dir, "hub")

# ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
os.makedirs(hf_cache_dir, exist_ok=True)
os.makedirs(os.path.join(hf_cache_dir, "transformers"), exist_ok=True)
os.makedirs(os.path.join(hf_cache_dir, "datasets"), exist_ok=True)
os.makedirs(os.path.join(hf_cache_dir, "hub"), exist_ok=True)

import json
import yaml
import math
import torch
import numpy as np
import argparse
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, fields
from pathlib import Path
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, EvalPrediction
)
from datasets import load_dataset, DatasetDict
from peft import (
    LoraConfig, AdaLoraConfig, get_peft_model, 
    TaskType, PeftModel, PeftConfig
)
from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef
import logging

# å¯¼å…¥å¯è§†åŒ–å·¥å…·
import sys
sys.path.append(os.path.join(project_root, 'tools'))
try:
    from benchmarks.visualization import BenchmarkVisualizer
    from benchmarks.base_benchmark import BaseBenchmark, BenchmarkConfig, BenchmarkResult
    from matplotlib_utils import setup_chinese_font
    from metrics_collector import MetricsCollector
    VISUALIZATION_AVAILABLE = True
except ImportError as e:
    # åœ¨loggeré…ç½®ä¹‹å‰ï¼Œä½¿ç”¨printè¾“å‡ºè­¦å‘Š
    print(f"Warning: Failed to import visualization tools: {e}")
    VISUALIZATION_AVAILABLE = False
    BenchmarkVisualizer = None
    setup_chinese_font = None
    MetricsCollector = None

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®ç±»"""
    model_name: str = 'bert-base-uncased'
    tasks: List[str] = None
    max_seq_length: int = 128  # æ”¹åä¸ºmax_seq_lengthä»¥ç¬¦åˆç”¨æˆ·è¦æ±‚
    batch_size: int = 100
    learning_rate: float = 2e-4
    num_epochs: int = 5  # epochs
    warmup_epochs: float = 1  # warmup_epochsï¼Œé»˜è®¤ä¸ºæ€»epochsçš„10%
    optimizer: str = 'adamw_hf'  # optimizerç±»å‹
    weight_decay: float = 0.01  # weight_decay
    lr_scheduler: str = 'linear'  # lr_schedulerç±»å‹
    output_dir: str = './results'
    seed: int = 42
    use_mixed_precision: bool = True  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    dataloader_num_workers: int = 2  # å‡å°‘æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹ä»¥é¿å…å†…å­˜é—®é¢˜
    enable_visualization: bool = True  # å¯ç”¨å¯è§†åŒ–åŠŸèƒ½
    gradient_accumulation_steps: int = 8  # æ”¯æŒä»YAMLè¦†ç›–æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    
    def __post_init__(self):
        if self.tasks is None:
            self.tasks = ["sst2", "mnli", "qnli", "rte", "cola"]

class GLUEDataProcessor:
    """GLUEæ•°æ®å¤„ç†å™¨"""
    
    TASK_CONFIGS = {
        "sst2": {
            "dataset_name": "glue",
            "dataset_config": "sst2",
            "text_columns": ["sentence"],
            "label_column": "label",
            "num_labels": 2,
            "metric": "accuracy"
        },
        "mnli": {
            "dataset_name": "glue", 
            "dataset_config": "mnli",
            "text_columns": ["premise", "hypothesis"],
            "label_column": "label",
            "num_labels": 3,
            "metric": "accuracy"
        },
        "qnli": {
            "dataset_name": "glue",
            "dataset_config": "qnli", 
            "text_columns": ["question", "sentence"],
            "label_column": "label",
            "num_labels": 2,
            "metric": "accuracy"
        },
        "rte": {
            "dataset_name": "glue",
            "dataset_config": "rte",
            "text_columns": ["sentence1", "sentence2"], 
            "label_column": "label",
            "num_labels": 2,
            "metric": "accuracy"
        },
        "cola": {
            "dataset_name": "glue",
            "dataset_config": "cola",
            "text_columns": ["sentence"],
            "label_column": "label", 
            "num_labels": 2,
            "metric": "matthews_correlation"
        }
    }
    
    def __init__(self, tokenizer, max_seq_length: int = 128):
        self.tokenizer = tokenizer
        self.max_length = max_seq_length
    
    def load_dataset(self, task: str) -> DatasetDict:
        """åŠ è½½æŒ‡å®šä»»åŠ¡çš„æ•°æ®é›†"""
        if task not in self.TASK_CONFIGS:
            raise ValueError(f"Unsupported task: {task}")
        
        config = self.TASK_CONFIGS[task]
        dataset = load_dataset(config["dataset_name"], config["dataset_config"])
        
        # å¯¹äºMNLIï¼Œåªä½¿ç”¨matchedéªŒè¯é›†
        if task == "mnli":
            dataset["validation"] = dataset["validation_matched"]
            del dataset["validation_matched"]
            del dataset["validation_mismatched"]
        
        return dataset
    
    def preprocess_function(self, examples, task: str):
        """é¢„å¤„ç†å‡½æ•°"""
        config = self.TASK_CONFIGS[task]
        text_columns = config["text_columns"]
        
        if len(text_columns) == 1:
            # å•å¥ä»»åŠ¡
            texts = examples[text_columns[0]]
            inputs = self.tokenizer(
                texts,
                truncation=True,
                padding="max_length",
                max_length=self.max_length,
                return_tensors=None
            )
        else:
            # å¥å­å¯¹ä»»åŠ¡
            texts1 = examples[text_columns[0]]
            texts2 = examples[text_columns[1]]
            inputs = self.tokenizer(
                texts1, texts2,
                truncation=True,
                padding="max_length",
                max_length=self.max_length,
                return_tensors=None
            )
        
        inputs["labels"] = examples[config["label_column"]]
        return inputs

class PEFTMethodFactory:
    """PEFTæ–¹æ³•å·¥å‚"""
    
    # å®šä¹‰ä¸åŒæ–¹æ³•çš„warmupç­–ç•¥
    WARMUP_STRATEGIES = {
        "lora": "lr_only",      # LoRAï¼šåªè¿›è¡Œå­¦ä¹ ç‡warmupï¼Œç®—æ³•æœºåˆ¶ä¸å˜
        "dora": "lr_only",      # DoRAï¼šåªè¿›è¡Œå­¦ä¹ ç‡warmupï¼Œç®—æ³•æœºåˆ¶ä¸å˜
        "adalora": "full",      # AdaLoRAï¼šéœ€è¦å®Œæ•´warmupï¼Œç¦ç”¨rankå˜åŒ–
        "dylora": "full",       # DyLoRAï¼šéœ€è¦å®Œæ•´warmupï¼Œç¦ç”¨åŠ¨æ€æœºåˆ¶
        "loraven": "full",      # LoRAvenï¼šéœ€è¦å®Œæ•´warmupï¼Œç¦ç”¨å‰ªæç­‰åŠ¨æ€æœºåˆ¶
        "full_finetune": "lr_only"  # å…¨å‚æ•°å¾®è°ƒï¼šåªè¿›è¡Œå­¦ä¹ ç‡warmup
    }
    
    @staticmethod
    def get_warmup_strategy(method: str) -> str:
        """è·å–æŒ‡å®šæ–¹æ³•çš„warmupç­–ç•¥"""
        return PEFTMethodFactory.WARMUP_STRATEGIES.get(method.lower(), "lr_only")
    
    @staticmethod
    def should_disable_dynamic_mechanisms(method: str, current_step: int, warmup_steps: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç¦ç”¨åŠ¨æ€æœºåˆ¶"""
        strategy = PEFTMethodFactory.get_warmup_strategy(method)
        if strategy == "full" and current_step < warmup_steps:
            return True
        return False
    
    @staticmethod
    def create_lora_config(num_labels: int, overrides: Optional[Dict[str, Any]] = None) -> LoraConfig:
        """åˆ›å»ºLoRAé…ç½®ï¼Œæ”¯æŒYAMLè¦†ç›–"""
        cfg = dict(
            r=8,
            lora_alpha=32,
            target_modules=["query_proj", "key_proj", "value_proj", "dense"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.SEQ_CLS,
        )
        overrides = overrides or {}
        cfg.update(overrides)
        return LoraConfig(**cfg)
    
    @staticmethod
    def create_adalora_config(num_labels: int, overrides: Optional[Dict[str, Any]] = None) -> AdaLoraConfig:
        """åˆ›å»ºAdaLoRAé…ç½®ï¼Œæ”¯æŒYAMLè¦†ç›–"""
        cfg = dict(
            r=8,
            lora_alpha=32,
            target_modules=["query_proj", "key_proj", "value_proj", "dense"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.SEQ_CLS,
            init_r=12,
            target_r=4,
            beta1=0.85,
            beta2=0.85,
            tinit=100,
            tfinal=500,
            deltaT=10,
            total_step=1000,
        )
        overrides = overrides or {}
        cfg.update(overrides)
        return AdaLoraConfig(**cfg)
    
    @staticmethod
    def create_dora_config(num_labels: int, overrides: Optional[Dict[str, Any]] = None) -> LoraConfig:
        """åˆ›å»ºDoRAé…ç½®ï¼Œæ”¯æŒYAMLè¦†ç›–"""
        cfg = dict(
            r=8,
            lora_alpha=32,
            target_modules=["query_proj", "key_proj", "value_proj", "dense"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.SEQ_CLS,
            use_dora=True,
        )
        overrides = overrides or {}
        cfg.update(overrides)
        return LoraConfig(**cfg)

class GLUEEvaluator:
    """GLUEè¯„ä¼°å™¨"""
    
    @staticmethod
    def compute_metrics(eval_pred: EvalPrediction, task: str) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡å¹¶æ·»åŠ eval_å‰ç¼€"""
        try:
            # æ­£ç¡®å¤„ç†EvalPredictionå¯¹è±¡
            if hasattr(eval_pred, 'predictions') and hasattr(eval_pred, 'label_ids'):
                predictions = eval_pred.predictions
                labels = eval_pred.label_ids
            else:
                predictions, labels = eval_pred
            
            # ç¡®ä¿æ˜¯numpyæ•°ç»„
            if torch.is_tensor(predictions):
                predictions = predictions.cpu().numpy()
            if torch.is_tensor(labels):
                labels = labels.cpu().numpy()
                
            # è·å–é¢„æµ‹ç±»åˆ«
            if predictions.ndim > 1:
                predictions = np.argmax(predictions, axis=1)
            
            if task == "cola":
                # CoLAä½¿ç”¨Matthewsç›¸å…³ç³»æ•°
                return {
                    "eval_matthews_correlation": matthews_corrcoef(labels, predictions),
                    "eval_accuracy": accuracy_score(labels, predictions)
                }
            else:
                # å…¶ä»–ä»»åŠ¡ä½¿ç”¨å‡†ç¡®ç‡
                return {
                    "eval_accuracy": accuracy_score(labels, predictions),
                    "eval_f1": f1_score(labels, predictions, average="macro")
                }
        except Exception as e:
            logger.error(f"compute_metricså‡½æ•°ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # è¿”å›åŒ…å«eval_accuracyçš„é»˜è®¤æŒ‡æ ‡ï¼Œé¿å…è®­ç»ƒä¸­æ–­
            return {"eval_accuracy": 0.0}

class GLUEBenchmark:
    """GLUEåŸºå‡†æµ‹è¯•ä¸»ç±»"""
    
    def __init__(self, config: ExperimentConfig, yaml_overrides: Optional[Dict[str, Any]] = None):
        # æ”¯æŒä»YAMLåˆå¹¶è¦†ç›–çš„é…ç½®
        self.base_config = config
        self.yaml_overrides = yaml_overrides or {}
        # æ ¹æ®æ¨¡å‹/ä»»åŠ¡/æ–¹æ³•åŠ¨æ€ç”Ÿæˆè¿è¡Œæ—¶é…ç½®ï¼Œé»˜è®¤å…ˆä½¿ç”¨base_config
        self.config = config
        self.tokenizer = None
        self.data_processor = None
        self.evaluator = GLUEEvaluator()
        self.results = {}
        
        # åˆå§‹åŒ–è¯¦ç»†æŒ‡æ ‡æ”¶é›†å™¨
        if VISUALIZATION_AVAILABLE and MetricsCollector:
            try:
                self.metrics_collector = MetricsCollector(config.output_dir)
                logger.info("Metrics collector initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize metrics collector: {e}")
                self.metrics_collector = None
        else:
            self.metrics_collector = None
        
        # åˆå§‹åŒ–å¯è§†åŒ–å™¨
        if config.enable_visualization and VISUALIZATION_AVAILABLE:
            try:
                # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
                if setup_chinese_font:
                    setup_chinese_font()
                self.visualizer = BenchmarkVisualizer(Path(config.output_dir))
                logger.info("Visualization tools initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize visualization tools: {e}")
                self.visualizer = None
        else:
            if config.enable_visualization and not VISUALIZATION_AVAILABLE:
                logger.warning("Visualization requested but tools not available")
            self.visualizer = None
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(config.output_dir, exist_ok=True)
    
    def _compose_runtime_config(self, method: str, task: str) -> ExperimentConfig:
        """æ ¹æ®YAMLè¦†ç›–ç”Ÿæˆå½“å‰æ¨¡å‹/ä»»åŠ¡/æ–¹æ³•çš„å±€éƒ¨é…ç½®"""
        cfg_dict = {
            'model_name': self.base_config.model_name,
            'tasks': [task],
            'max_seq_length': self.base_config.max_seq_length,
            'batch_size': self.base_config.batch_size,
            'learning_rate': self.base_config.learning_rate,
            'num_epochs': self.base_config.num_epochs,
            'warmup_epochs': self.base_config.warmup_epochs,
            'optimizer': self.base_config.optimizer,
            'weight_decay': self.base_config.weight_decay,
            'lr_scheduler': self.base_config.lr_scheduler,
            'output_dir': self.base_config.output_dir,
            'seed': self.base_config.seed,
            'use_mixed_precision': self.base_config.use_mixed_precision,
            'dataloader_num_workers': self.base_config.dataloader_num_workers,
            'enable_visualization': self.base_config.enable_visualization,
        }

        # YAMLå±‚çº§ï¼šglobal -> models[model_name] -> tasks[task] -> methods[method]
        def deep_update(dst: Dict[str, Any], src: Dict[str, Any]):
            for k, v in src.items():
                if isinstance(v, dict) and isinstance(dst.get(k), dict):
                    deep_update(dst[k], v)
                else:
                    dst[k] = v

        # é€å±‚åˆå¹¶ï¼Œä»…åˆå¹¶trainingå­å­—æ®µï¼Œé¿å…å°†peftç­‰æœªçŸ¥é”®æ³¨å…¥ExperimentConfig
        global_training = self.yaml_overrides.get('global', {}).get('training', {})
        model_training = self.yaml_overrides.get('models', {}).get(self.base_config.model_name, {}).get('training', {})
        task_training = self.yaml_overrides.get('models', {}).get(self.base_config.model_name, {}).get('tasks', {}).get(task, {}).get('training', {})
        method_training = self.yaml_overrides.get('models', {}).get(self.base_config.model_name, {}).get('tasks', {}).get(task, {}).get('methods', {}).get(method, {}).get('training', {})

        deep_update(cfg_dict, global_training)
        deep_update(cfg_dict, model_training)
        deep_update(cfg_dict, task_training)
        deep_update(cfg_dict, method_training)

        # è¿‡æ»¤æœªçŸ¥é”®ï¼Œç¡®ä¿åªä¼ å…¥ExperimentConfigå®šä¹‰çš„å­—æ®µ
        allowed_keys = {f.name for f in fields(ExperimentConfig)}
        filtered_cfg = {k: v for k, v in cfg_dict.items() if k in allowed_keys}

        # ç”Ÿæˆæ–°çš„ExperimentConfig
        runtime_config = ExperimentConfig(**filtered_cfg)
        return runtime_config

    def _get_peft_overrides(self, method: str, task: str) -> Dict[str, Any]:
        """ç»„åˆä¸åŒå±‚çº§çš„PEFTè¦†ç›–é…ç½®"""
        def deep_update(dst: Dict[str, Any], src: Dict[str, Any]):
            for k, v in src.items():
                if isinstance(v, dict) and isinstance(dst.get(k), dict):
                    deep_update(dst[k], v)
                else:
                    dst[k] = v
        overrides: Dict[str, Any] = {}
        global_peft = self.yaml_overrides.get('global', {}).get('peft', {})
        model_peft = self.yaml_overrides.get('models', {}).get(self.base_config.model_name, {}).get('peft', {})
        task_peft = self.yaml_overrides.get('models', {}).get(self.base_config.model_name, {}).get('tasks', {}).get(task, {}).get('peft', {})
        method_peft = self.yaml_overrides.get('models', {}).get(self.base_config.model_name, {}).get('tasks', {}).get(task, {}).get('methods', {}).get(method, {}).get('peft', {})
        deep_update(overrides, global_peft)
        deep_update(overrides, model_peft)
        deep_update(overrides, task_peft)
        deep_update(overrides, method_peft)
        return overrides

    def run_experiment(self, method: str, task: str) -> Dict[str, float]:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        logger.info(f"Running {method} on {task}")

        # æ ¹æ®YAMLè¦†ç›–ç»„åˆè¿è¡Œæ—¶é…ç½®
        self.config = self._compose_runtime_config(method, task)

        # ç¡®ä¿tokenizerå’Œæ•°æ®å¤„ç†å™¨ä¸å½“å‰é…ç½®ä¸€è‡´
        if self.tokenizer is None or getattr(self.base_config, 'model_name', None) != self.config.model_name:
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
        # é‡å»ºæ•°æ®å¤„ç†å™¨ä»¥åº”ç”¨å½“å‰max_seq_length
        self.data_processor = GLUEDataProcessor(self.tokenizer, self.config.max_seq_length)
        
        # æ£€æŸ¥å¤šGPUå¯ç”¨æ€§
        if torch.cuda.device_count() > 1:
            logger.info(f"Using {torch.cuda.device_count()} GPUs for parallel training")
            device = torch.device("cuda")
        else:
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {device}")
        
        # åŠ è½½æ•°æ®é›†
        dataset = self.data_processor.load_dataset(task)
        task_config = self.data_processor.TASK_CONFIGS[task]
        
        # é¢„å¤„ç†æ•°æ®
        tokenized_dataset = dataset.map(
            lambda examples: self.data_processor.preprocess_function(examples, task),
            batched=True,
            remove_columns=dataset["train"].column_names,
            num_proc=4,  # å‡å°‘å¹¶è¡Œè¿›ç¨‹æ•°ä»¥é¿å…å†…å­˜é—®é¢˜
            load_from_cache_file=True,  # å¯ç”¨ç¼“å­˜ä»¥åŠ é€Ÿé‡å¤å¤„ç†
            desc=f"Tokenizing {task} dataset"  # æ·»åŠ è¿›åº¦æè¿°
        )
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModelForSequenceClassification.from_pretrained(
            self.config.model_name,
            num_labels=task_config["num_labels"]
        )
        
        # åº”ç”¨PEFTæ–¹æ³•æˆ–å…¨å‚æ•°å¾®è°ƒ
        if method == "lora":
            peft_config = PEFTMethodFactory.create_lora_config(task_config["num_labels"], overrides=self._get_peft_overrides(method, task))
            model = get_peft_model(model, peft_config)
            model.print_trainable_parameters()
        elif method == "adalora":
            peft_config = PEFTMethodFactory.create_adalora_config(task_config["num_labels"], overrides=self._get_peft_overrides(method, task))
            model = get_peft_model(model, peft_config)
            model.print_trainable_parameters()
        elif method == "dora":
            peft_config = PEFTMethodFactory.create_dora_config(task_config["num_labels"], overrides=self._get_peft_overrides(method, task))
            model = get_peft_model(model, peft_config)
            model.print_trainable_parameters()
        elif method == "loraven":
            # ä½¿ç”¨LoRAvené…ç½®ï¼ˆéœ€è¦ä»loraven.peft_adapterå¯¼å…¥ï¼‰
            try:
                from loraven.peft_adapter import LoRAvenConfig
                loraven_defaults = dict(
                    r_min=4,
                    r_max=16,
                    r=8,
                    lora_alpha=32,
                    target_modules=["query_proj", "key_proj", "value_proj", "dense"],
                    lora_dropout=0.1,
                    bias="none",
                    task_type=TaskType.SEQ_CLS,
                    complexity_scorer_type="lightweight",
                    rank_scheduler_type="linear",
                    energy_budget=1000.0
                )
                overrides = self._get_peft_overrides(method, task)
                loraven_defaults.update(overrides)
                peft_config = LoRAvenConfig(**loraven_defaults)
                model = get_peft_model(model, peft_config)
                model.print_trainable_parameters()
            except ImportError:
                logger.warning("LoRAven not available, using LoRA instead")
                peft_config = PEFTMethodFactory.create_lora_config(task_config["num_labels"])
                model = get_peft_model(model, peft_config)
                model.print_trainable_parameters()
        elif method == "full_finetune":
            # å…¨å‚æ•°å¾®è°ƒï¼šä¸ä½¿ç”¨PEFTï¼Œç›´æ¥è®­ç»ƒæ•´ä¸ªæ¨¡å‹
            logger.info("Using full fine-tuning (all parameters trainable)")
            # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            logger.info(f"Total parameters: {total_params:,}")
            logger.info(f"Trainable parameters: {trainable_params:,}")
            logger.info(f"Trainable parameters percentage: {100 * trainable_params / total_params:.2f}%")
        else:
            raise ValueError(f"Unsupported method: {method}")
        
        # åˆ›å»ºPEFTæ¨¡å‹ï¼ˆä»…å¯¹PEFTæ–¹æ³•ï¼‰
        # æ³¨æ„ï¼šfull_finetuneæ–¹æ³•ä¸éœ€è¦get_peft_modelåŒ…è£…
        
        # å¤šGPUå¹¶è¡Œè®­ç»ƒé…ç½®
        if torch.cuda.device_count() > 1:
            logger.info(f"Using {torch.cuda.device_count()} GPUs for parallel training")
            # ä½¿ç”¨DataParallelè¿›è¡Œå¤šGPUè®­ç»ƒï¼Œé¿å…æ ‡é‡å¼ é‡gatherè­¦å‘Š
            model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))
            model = model.to(device)
        else:
            model = model.to(device)
        
        # æ ¹æ®ä»»åŠ¡ç¡®å®šæœ€ä½³æ¨¡å‹æŒ‡æ ‡
        if task == "cola":
            metric_for_best_model = "eval_matthews_correlation"  # æ¢å¤eval_å‰ç¼€
        else:
            metric_for_best_model = "eval_accuracy"  # æ¢å¤eval_å‰ç¼€
        
        # è®¡ç®—warmup_stepsï¼ˆä»warmup_epochsè½¬æ¢ï¼‰
        # ä¼°ç®—æ¯ä¸ªepochçš„æ­¥æ•°ï¼šæ•°æ®é›†å¤§å° / (batch_size * GPUæ•°é‡)
        train_dataset_size = len(tokenized_dataset["train"])
        effective_batch_size = self.config.batch_size * max(1, torch.cuda.device_count())
        steps_per_epoch = train_dataset_size // effective_batch_size
        
        # è®¡ç®—ä¼˜åŒ–å™¨æ­¥æ•°ï¼ˆè€ƒè™‘æ¢¯åº¦ç´¯ç§¯ï¼‰
        gradient_accumulation_steps = int(getattr(self.config, 'gradient_accumulation_steps', 8))
        optimizer_steps_per_epoch = math.ceil(steps_per_epoch / gradient_accumulation_steps)
        
        # æ”¹è¿›çš„warmup_epochsé€»è¾‘ï¼š
        # - å¦‚æœwarmup_epochs >= 1ï¼Œåˆ™è§†ä¸ºå…·ä½“çš„epochæ•°
        # - å¦‚æœwarmup_epochs < 1ï¼Œåˆ™è§†ä¸ºæ€»epochsçš„æ¯”ä¾‹
        # - warmup_stepsåŸºäºä¼˜åŒ–å™¨æ­¥æ•°è®¡ç®—ï¼Œè€Œéæ•°æ®è¿­ä»£æ­¥æ•°
        if self.config.warmup_epochs >= 1:
            # æ•´æ•°è¡¨ç¤ºå…·ä½“çš„warmupè½®æ•°
            warmup_steps = int(self.config.warmup_epochs * optimizer_steps_per_epoch)
            logger.info(f"Using absolute warmup epochs: {self.config.warmup_epochs}")
        else:
            # å°æ•°è¡¨ç¤ºæ€»è½®æ•°çš„æ¯”ä¾‹
            warmup_steps = int(self.config.warmup_epochs * self.config.num_epochs * optimizer_steps_per_epoch)
            logger.info(f"Using proportional warmup epochs: {self.config.warmup_epochs} of total {self.config.num_epochs} epochs")
        
        logger.info(f"Training steps calculation: train_size={train_dataset_size}, effective_batch_size={effective_batch_size}")
        logger.info(f"Steps per epoch: {steps_per_epoch} (data iterations), {optimizer_steps_per_epoch} (optimizer steps)")
        logger.info(f"Gradient accumulation steps: {gradient_accumulation_steps}")
        logger.info(f"Calculated warmup_steps: {warmup_steps} (based on optimizer steps)")
        
        # æ ¹æ®PEFTæ–¹æ³•è°ƒæ•´warmupç­–ç•¥
        warmup_strategy = PEFTMethodFactory.get_warmup_strategy(method)
        logger.info(f"Method {method} uses warmup strategy: {warmup_strategy}")
        
        # å¯¹äºåªéœ€è¦å­¦ä¹ ç‡warmupçš„æ–¹æ³•ï¼ˆLoRA/DoRAï¼‰ï¼Œå¦‚æœwarmup_epochs=1ï¼Œåˆ™æœ‰æ•ˆ
        if warmup_strategy == "lr_only" and self.config.warmup_epochs == 1:
            logger.info(f"Enabling learning rate warmup for {method} (warmup_epochs=1)")
        elif warmup_strategy == "lr_only" and self.config.warmup_epochs != 1:
            # å¯¹äºLoRA/DoRAï¼Œå¦‚æœä¸æ˜¯1ï¼Œåˆ™ç¦ç”¨warmup
            warmup_steps = 0
            logger.info(f"Disabling warmup for {method} (warmup_epochs != 1)")
        
        logger.info(f"Training dataset size: {train_dataset_size}")
        logger.info(f"Effective batch size: {effective_batch_size}")
        logger.info(f"Steps per epoch: {steps_per_epoch}")
        logger.info(f"Final warmup steps: {warmup_steps}")
        
        # è®­ç»ƒå‚æ•°é…ç½®
        training_args = TrainingArguments(
            output_dir=f"{self.config.output_dir}/{method}_{task}",
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            learning_rate=self.config.learning_rate,
            warmup_steps=warmup_steps,  # ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„warmup_steps
            weight_decay=self.config.weight_decay,  # æ·»åŠ weight_decayå‚æ•°
            optim=self.config.optimizer,  # æ·»åŠ optimizerå‚æ•°
            lr_scheduler_type=self.config.lr_scheduler,  # æ·»åŠ lr_schedulerå‚æ•°
            logging_steps=100,
            eval_strategy="epoch",  # ä½¿ç”¨æ–°çš„å‚æ•°å
            save_strategy="no",  # ç¦ç”¨checkpointä¿å­˜ä»¥é¿å…modules_to_saveé”™è¯¯
            save_steps=1000000,  # è®¾ç½®å¾ˆå¤§çš„å€¼ç¡®ä¿ä¸ä¿å­˜
            load_best_model_at_end=False,  # ç¦ç”¨æœ€ä½³æ¨¡å‹åŠ è½½ä»¥é¿å…ä¿å­˜ç›¸å…³é”™è¯¯
            metric_for_best_model=None,  # æ˜¾å¼ç¦ç”¨æœ€ä½³æ¨¡å‹æŒ‡æ ‡æ£€æŸ¥
            greater_is_better=True,
            seed=self.config.seed,
            dataloader_num_workers=self.config.dataloader_num_workers,
            # æ··åˆç²¾åº¦è®­ç»ƒé…ç½®
            fp16=self.config.use_mixed_precision,
            gradient_accumulation_steps=gradient_accumulation_steps,  # YAMLå¯è¦†ç›–
            dataloader_pin_memory=True,    # å¯ç”¨pin_memoryä»¥æé«˜æ€§èƒ½
            remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰åˆ—ï¼Œé¿å…åˆ—ååŒ¹é…é—®é¢˜
            # å¤šGPUä¼˜åŒ–
            ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None,
            report_to=[],  # ç¦ç”¨æŠ¥å‘Šä»¥é¿å…é¢å¤–ä¾èµ–
            run_name=f"{method}_{task}",  # è®¾ç½®è¿è¡Œåç§°
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        # å®šä¹‰compute_metricså‡½æ•°ï¼Œç¡®ä¿æ­£ç¡®è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        def compute_metrics(eval_pred):
            """è®¡ç®—è¯„ä¼°æŒ‡æ ‡å¹¶æ·»åŠ eval_å‰ç¼€"""
            try:
                from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef
                import numpy as np
                
                logger.info(f"compute_metrics called for task: {task}")
                
                # æ­£ç¡®å¤„ç†EvalPredictionå¯¹è±¡
                if hasattr(eval_pred, 'predictions') and hasattr(eval_pred, 'label_ids'):
                    predictions = eval_pred.predictions
                    labels = eval_pred.label_ids
                else:
                    # å¤„ç†tupleæ ¼å¼çš„è¾“å…¥
                    if isinstance(eval_pred, tuple) and len(eval_pred) == 2:
                        predictions, labels = eval_pred
                    else:
                        predictions = eval_pred
                        labels = None
                
                logger.info(f"Predictions shape: {predictions.shape if hasattr(predictions, 'shape') else type(predictions)}")
                logger.info(f"Labels shape: {labels.shape if hasattr(labels, 'shape') else type(labels)}")
                
                # ç¡®ä¿æ˜¯numpyæ•°ç»„
                if torch.is_tensor(predictions):
                    predictions = predictions.cpu().numpy()
                if torch.is_tensor(labels):
                    labels = labels.cpu().numpy()
                
                # å¤„ç†predictionså¯èƒ½æ˜¯tupleçš„æƒ…å†µ
                if isinstance(predictions, tuple):
                    # é€šå¸¸ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯logits
                    predictions = predictions[0]
                    if torch.is_tensor(predictions):
                        predictions = predictions.cpu().numpy()
                
                # ç¡®ä¿predictionså’Œlabelséƒ½æ˜¯numpyæ•°ç»„
                if torch.is_tensor(predictions):
                    predictions = predictions.cpu().numpy()
                if torch.is_tensor(labels):
                    labels = labels.cpu().numpy()
                    
                logger.info(f"After processing - Predictions shape: {predictions.shape if hasattr(predictions, 'shape') else type(predictions)}")
                logger.info(f"After processing - Labels shape: {labels.shape if hasattr(labels, 'shape') else type(labels)}")
                    
                # è·å–é¢„æµ‹ç±»åˆ«
                if hasattr(predictions, 'ndim') and predictions.ndim > 1:
                    logger.info(f"Applying argmax to predictions with shape: {predictions.shape}")
                    predictions = np.argmax(predictions, axis=1)
                    logger.info(f"After argmax - Predictions shape: {predictions.shape}")
                else:
                    logger.info(f"Predictions already 1D or no ndim attribute: {predictions.shape if hasattr(predictions, 'shape') else type(predictions)}")
                
                if task == "cola":
                    # CoLAä½¿ç”¨Matthewsç›¸å…³ç³»æ•°
                    accuracy = accuracy_score(labels, predictions)
                    matthews_corr = matthews_corrcoef(labels, predictions)
                    result = {
                        "eval_accuracy": accuracy,
                        "eval_matthews_correlation": matthews_corr
                    }
                else:
                    # å…¶ä»–ä»»åŠ¡ä½¿ç”¨å‡†ç¡®ç‡å’ŒF1
                    accuracy = accuracy_score(labels, predictions)
                    f1 = f1_score(labels, predictions, average="macro")
                    result = {
                        "eval_accuracy": accuracy,
                        "eval_f1": f1
                    }
                
                logger.info(f"Computed metrics for {task}: {result}")
                return result
                
            except Exception as e:
                logger.error(f"Error in compute_metrics: {e}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
                # è¿”å›åŒ…å«eval_accuracyçš„é»˜è®¤æŒ‡æ ‡ï¼Œé¿å…è®­ç»ƒä¸­æ–­
                return {"eval_accuracy": 0.0}
        
        # åˆ›å»ºæ–¹æ³•æ„ŸçŸ¥çš„Trainerç±»
        class MethodAwareTrainer(Trainer):
            def __init__(self, peft_method, warmup_steps, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.peft_method = peft_method
                self.warmup_steps = warmup_steps
                self.current_step = 0
                
            def training_step(self, model, inputs, num_items_in_batch=None):
                # åœ¨è®­ç»ƒæ­¥éª¤ä¸­æ£€æŸ¥æ˜¯å¦éœ€è¦ç¦ç”¨åŠ¨æ€æœºåˆ¶
                if PEFTMethodFactory.should_disable_dynamic_mechanisms(
                    self.peft_method, self.current_step, self.warmup_steps
                ):
                    # å¯¹äºéœ€è¦å®Œæ•´warmupçš„æ–¹æ³•ï¼Œåœ¨warmupæœŸé—´ç¦ç”¨åŠ¨æ€æœºåˆ¶
                    # è¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“çš„PEFTæ–¹æ³•å®ç°ç›¸åº”çš„ç¦ç”¨é€»è¾‘
                    # ä¾‹å¦‚ï¼šæš‚æ—¶ç¦ç”¨AdaLoRAçš„rankå˜åŒ–ã€DyLoRAçš„åŠ¨æ€æœºåˆ¶ç­‰
                    logger.info(f"Step {self.current_step}: Disabling dynamic mechanisms for {self.peft_method} during warmup")
                
                result = super().training_step(model, inputs, num_items_in_batch)
                self.current_step += 1
                return result
        
        trainer = MethodAwareTrainer(
            peft_method=method,
            warmup_steps=warmup_steps,
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset["train"],
            eval_dataset=tokenized_dataset["validation"],
            compute_metrics=compute_metrics,
        )
        
        # è®­ç»ƒ
        trainer.train()
        
        # è¯„ä¼°
        try:
            eval_results = trainer.evaluate()
        except KeyError as e:
            # æ•è·å› metric_for_best_modelå¯¼è‡´çš„KeyError
            logger.error(f"trainer.evaluate() raised KeyError: {e}. Falling back to manual metrics computation.")
            eval_results = {}

        # æŒ‡æ ‡å›é€€ï¼šè‹¥æœªè¿”å›æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼Œåˆ™ä½¿ç”¨predictæ‰‹åŠ¨è®¡ç®—å¹¶è¡¥å……
        expected_key = "eval_matthews_correlation" if task == "cola" else "eval_accuracy"
        if expected_key not in eval_results:
            logger.warning(f"{expected_key} not found in eval_results; computing metrics via predict() fallback")
            try:
                # ç¡®ä¿éªŒè¯é›†åŒ…å«æ ‡ç­¾
                eval_dataset = tokenized_dataset["validation"]
                pred_output = trainer.predict(eval_dataset)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾
                if pred_output.label_ids is not None:
                    # ä½¿ç”¨ä¸Trainerä¸€è‡´çš„compute_metricsé€»è¾‘
                    computed_metrics = compute_metrics((pred_output.predictions, pred_output.label_ids))
                    if isinstance(computed_metrics, dict):
                        eval_results.update(computed_metrics)
                        logger.info(f"Manually computed metrics added: {computed_metrics}")
                    else:
                        logger.warning("compute_metrics did not return a dict; skipping manual metric update")
                else:
                    # å¦‚æœpredictæ²¡æœ‰è¿”å›æ ‡ç­¾ï¼Œç›´æ¥ä»æ•°æ®é›†è·å–
                    logger.warning("pred_output.label_ids is None, extracting labels from dataset")
                    if "labels" in eval_dataset.column_names:
                        true_labels = np.array(eval_dataset["labels"])
                        logger.info(f"Extracted labels shape: {true_labels.shape}")
                        logger.info(f"Predictions type: {type(pred_output.predictions)}")
                        logger.info(f"Predictions shape: {pred_output.predictions.shape if hasattr(pred_output.predictions, 'shape') else 'No shape attribute'}")
                        
                        # ç¡®ä¿predictionsæ˜¯å®Œæ•´çš„é¢„æµ‹ç»“æœ
                        predictions = pred_output.predictions
                        
                        # å¤„ç†predictionså¯èƒ½æ˜¯tupleçš„æƒ…å†µ
                        if isinstance(predictions, tuple):
                            logger.info(f"Predictions is tuple with length: {len(predictions)}")
                            for i, elem in enumerate(predictions):
                                logger.info(f"Tuple element {i}: type={type(elem)}, shape={elem.shape if hasattr(elem, 'shape') else 'No shape'}")
                            
                            # æ™ºèƒ½é€‰æ‹©æ­£ç¡®çš„logitså…ƒç´ 
                            # å¯»æ‰¾å½¢çŠ¶ä¸º(æ ·æœ¬æ•°, ç±»åˆ«æ•°)çš„å…ƒç´ 
                            target_samples = true_labels.shape[0]
                            logits_elem = None
                            
                            for i, elem in enumerate(predictions):
                                if hasattr(elem, 'shape') and len(elem.shape) == 2:
                                    if elem.shape[0] == target_samples:
                                        logits_elem = elem
                                        logger.info(f"Found logits at tuple element {i} with shape {elem.shape}")
                                        break
                            
                            if logits_elem is not None:
                                predictions = logits_elem
                            else:
                                # å›é€€åˆ°ç¬¬ä¸€ä¸ªå…ƒç´ 
                                predictions = predictions[0]
                                logger.warning("Could not find matching logits element, using first element")
                            
                            logger.info(f"Selected predictions shape: {predictions.shape if hasattr(predictions, 'shape') else type(predictions)}")
                        
                        if torch.is_tensor(predictions):
                            predictions = predictions.cpu().numpy()
                        
                        logger.info(f"After tensor conversion - Predictions shape: {predictions.shape if hasattr(predictions, 'shape') else type(predictions)}")
                        
                        # æ£€æŸ¥predictionsçš„ç»´åº¦å’Œå†…å®¹
                        if hasattr(predictions, 'shape'):
                            logger.info(f"Predictions content (first 10): {predictions[:10] if len(predictions) > 10 else predictions}")
                            
                            # å¦‚æœpredictionså½¢çŠ¶ä¸åŒ¹é…ï¼Œå¯èƒ½éœ€è¦é‡æ–°è·å–å®Œæ•´çš„é¢„æµ‹ç»“æœ
                            if predictions.shape[0] != true_labels.shape[0]:
                                logger.warning(f"Predictions shape {predictions.shape} doesn't match labels shape {true_labels.shape}")
                                logger.info("Attempting to get full predictions by running model inference...")
                                
                                # å°è¯•ç›´æ¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
                                try:
                                    model = trainer.model
                                    model.eval()
                                    all_predictions = []
                                    
                                    # ä½¿ç”¨DataLoaderè¿›è¡Œæ‰¹é‡æ¨ç†
                                    eval_dataloader = trainer.get_eval_dataloader(eval_dataset)
                                    
                                    with torch.no_grad():
                                        for batch in eval_dataloader:
                                            # å°†batchç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
                                            batch = {k: v.to(trainer.args.device) for k, v in batch.items() if k != 'labels'}
                                            outputs = model(**batch)
                                            logits = outputs.logits
                                            predictions_batch = torch.argmax(logits, dim=-1)
                                            all_predictions.extend(predictions_batch.cpu().numpy())
                                    
                                    predictions = np.array(all_predictions)
                                    logger.info(f"Full inference predictions shape: {predictions.shape}")
                                    
                                except Exception as e:
                                    logger.error(f"Full inference failed: {e}")
                                    eval_results[expected_key] = 0.0
                        
                        # å¦‚æœpredictionsæ˜¯2Dçš„logitsï¼Œéœ€è¦åº”ç”¨argmax
                        if hasattr(predictions, 'ndim') and predictions.ndim > 1:
                            logger.info(f"Applying argmax to predictions with shape: {predictions.shape}")
                            predictions = np.argmax(predictions, axis=1)
                            logger.info(f"After argmax - Predictions shape: {predictions.shape}")
                        
                        # ç›´æ¥è®¡ç®—å‡†ç¡®ç‡ï¼Œä¸ä½¿ç”¨compute_metricså‡½æ•°
                        if hasattr(predictions, 'shape') and predictions.shape[0] == true_labels.shape[0]:
                            try:
                                accuracy = accuracy_score(true_labels, predictions)
                                computed_metrics = {expected_key: accuracy}
                                eval_results.update(computed_metrics)
                                logger.info(f"Manually computed metrics added: {computed_metrics}")
                            except Exception as e:
                                logger.error(f"Manual metric computation failed: {e}")
                                eval_results[expected_key] = 0.0
                        else:
                            logger.error(f"Shape mismatch or invalid predictions: predictions {predictions.shape if hasattr(predictions, 'shape') else type(predictions)}, labels {true_labels.shape}")
                            eval_results[expected_key] = 0.0
                    else:
                        logger.error("No labels found in validation dataset")
                        eval_results[expected_key] = 0.0
                        
            except Exception as e:
                logger.error(f"Manual metric computation failed: {e}")
                # å…œåº•ï¼šè‡³å°‘è¡¥ä¸Šæ ¸å¿ƒæŒ‡æ ‡é”®ï¼Œé¿å…åç»­æŠ¥è¡¨æˆ–é€‰æ‹©æœ€ä¼˜æ¨¡å‹æ—¶æŠ¥é”™
                eval_results[expected_key] = 0.0
        
        # ä¿å­˜æ¨¡å‹
        try:
            if method == "full_finetune":
                # å…¨å‚æ•°å¾®è°ƒï¼šä¿å­˜å®Œæ•´æ¨¡å‹
                model_output_dir = f"{self.config.output_dir}/{method}_{task}_model"
                
                # å¤„ç†DataParallelåŒ…è£…çš„æ¨¡å‹
                if hasattr(model, 'module'):
                    model_to_save = model.module
                else:
                    model_to_save = model
                
                # åˆ›å»ºè¾“å‡ºç›®å½•
                import os
                os.makedirs(model_output_dir, exist_ok=True)
                
                # ä¿å­˜å®Œæ•´æ¨¡å‹
                model_to_save.save_pretrained(model_output_dir)
                logger.info(f"Full model saved to {model_output_dir}")
            else:
                # PEFTæ–¹æ³•ï¼šåªä¿å­˜é€‚é…å™¨
                adapter_output_dir = f"{self.config.output_dir}/{method}_{task}_adapter"
                
                # è·å–è¦ä¿å­˜çš„æ¨¡å‹
                model_to_save = model
                if hasattr(model, 'module'):
                    # å¦‚æœä½¿ç”¨äº†DataParallelï¼Œéœ€è¦å…ˆunwrap
                    model_to_save = model.module
                
                # åªä¿å­˜PEFTé€‚é…å™¨ï¼Œé¿å…å®Œæ•´æ¨¡å‹ä¿å­˜æ—¶çš„é”®åå†²çª
                if hasattr(model_to_save, 'save_pretrained'):
                    # åˆ›å»ºè¾“å‡ºç›®å½•
                    import os
                    os.makedirs(adapter_output_dir, exist_ok=True)
                    
                    # ä¿å­˜é€‚é…å™¨æƒé‡
                    model_to_save.save_pretrained(adapter_output_dir)
                    logger.info(f"PEFT adapter saved to {adapter_output_dir}")
                else:
                    logger.warning(f"Model does not have save_pretrained method")
                
        except Exception as e:
            logger.warning(f"Failed to save model: {e}")
            # ä¸è®©ä¿å­˜å¤±è´¥å½±å“è®­ç»ƒç»“æœ
            pass
        
        # ä¿å­˜ç»“æœ
        result_key = f"{method}_{task}"
        self.results[result_key] = eval_results
        
        # æ”¶é›†è¯¦ç»†æŒ‡æ ‡
        if self.metrics_collector:
            try:
                logger.info(f"Collecting detailed metrics for {method} on {task}")
                
                # å‡†å¤‡æ ·æœ¬è¾“å…¥ç”¨äºFLOPsè®¡ç®—
                sample_input = None
                if tokenized_dataset and "validation" in tokenized_dataset:
                    sample_batch = tokenized_dataset["validation"][:1]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                    sample_input = {
                        'input_ids': torch.tensor(sample_batch['input_ids']).to(model.device),
                        'attention_mask': torch.tensor(sample_batch['attention_mask']).to(model.device)
                    }
                
                # è·å–è®­ç»ƒå†å²ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                training_history = None
                if hasattr(trainer, 'state') and hasattr(trainer.state, 'log_history'):
                    training_history = trainer.state.log_history
                
                # è·å–éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºåŠ¨æ€æ€§åˆ†æï¼‰
                eval_dataloader = None
                if tokenized_dataset and "validation" in tokenized_dataset:
                    eval_dataloader = trainer.get_eval_dataloader(tokenized_dataset["validation"])
                
                # ç”Ÿæˆè¯¦ç»†æŒ‡æ ‡æŠ¥å‘Š
                detailed_metrics = self.metrics_collector.generate_comprehensive_report(
                    model=model,
                    method=method,
                    task=task,
                    eval_results=eval_results,
                    sample_input=sample_input,
                    training_history=training_history,
                    dataloader=eval_dataloader
                )
                
                # å°†è¯¦ç»†æŒ‡æ ‡æ·»åŠ åˆ°ç»“æœä¸­
                self.results[result_key].update(detailed_metrics)
                
                logger.info(f"Detailed metrics collected successfully for {method} on {task}")
                
            except Exception as e:
                logger.warning(f"Failed to collect detailed metrics for {method} on {task}: {e}")
                import traceback
                logger.warning(f"Traceback: {traceback.format_exc()}")
        
        # ç§»é™¤å®Œæ•´æ¨¡å‹ä¿å­˜ä»¥é¿å…modules_to_saveé”™è¯¯
        # model.save_pretrained(f"{self.config.output_dir}/{method}_{task}")
        
        # è¾“å‡ºè¯¦ç»†æŒ‡æ ‡
        self._print_experiment_metrics(method, task, eval_results, model, trainer)
        
        return eval_results
    
    def _print_experiment_metrics(self, method: str, task: str, eval_results: Dict[str, float], model, trainer):
        """æ‰“å°å®éªŒçš„è¯¦ç»†æŒ‡æ ‡"""
        print("\n" + "="*80)
        print(f"å®éªŒå®Œæˆ: {method.upper()} on {task.upper()}")
        print("="*80)
        
        # 1. åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
        print("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print("-" * 40)
        for key, value in eval_results.items():
            if key.startswith('eval_'):
                metric_name = key.replace('eval_', '').replace('_', ' ').title()
                if isinstance(value, float):
                    print(f"  {metric_name:<25}: {value:.4f}")
                else:
                    print(f"  {metric_name:<25}: {value}")
        
        # 2. æ¨¡å‹å‚æ•°ç»Ÿè®¡
        print("\nğŸ”§ æ¨¡å‹å‚æ•°:")
        print("-" * 40)
        try:
            if hasattr(model, 'module'):
                # å¤„ç†DataParallelåŒ…è£…çš„æ¨¡å‹
                model_to_analyze = model.module
            else:
                model_to_analyze = model
                
            total_params = sum(p.numel() for p in model_to_analyze.parameters())
            trainable_params = sum(p.numel() for p in model_to_analyze.parameters() if p.requires_grad)
            
            print(f"  {'æ€»å‚æ•°é‡':<25}: {total_params:,}")
            print(f"  {'å¯è®­ç»ƒå‚æ•°é‡':<25}: {trainable_params:,}")
            print(f"  {'å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹':<25}: {100 * trainable_params / total_params:.2f}%")
            
            # å¯¹äºPEFTæ–¹æ³•ï¼Œæ˜¾ç¤ºé¢å¤–ä¿¡æ¯
            if method != "full_finetune" and hasattr(model_to_analyze, 'print_trainable_parameters'):
                print(f"  {'PEFTæ–¹æ³•':<25}: {method}")
                
        except Exception as e:
            logger.warning(f"Failed to compute parameter statistics: {e}")
        
        # 3. è®­ç»ƒç»Ÿè®¡
        print("\nâ±ï¸ è®­ç»ƒç»Ÿè®¡:")
        print("-" * 40)
        try:
            if hasattr(trainer, 'state') and hasattr(trainer.state, 'log_history'):
                log_history = trainer.state.log_history
                if log_history:
                    # è·å–è®­ç»ƒæ—¶é—´
                    if 'train_runtime' in eval_results:
                        runtime = eval_results['train_runtime']
                        print(f"  {'è®­ç»ƒæ—¶é—´':<25}: {runtime:.2f} ç§’")
                    
                    # è·å–è®­ç»ƒæ­¥æ•°
                    total_steps = trainer.state.global_step
                    print(f"  {'è®­ç»ƒæ­¥æ•°':<25}: {total_steps}")
                    
                    # è·å–æœ€ç»ˆè®­ç»ƒæŸå¤±
                    train_losses = [log['train_loss'] for log in log_history if 'train_loss' in log]
                    if train_losses:
                        final_loss = train_losses[-1]
                        print(f"  {'æœ€ç»ˆè®­ç»ƒæŸå¤±':<25}: {final_loss:.4f}")
                    
                    # è·å–å­¦ä¹ ç‡ä¿¡æ¯
                    learning_rates = [log['learning_rate'] for log in log_history if 'learning_rate' in log]
                    if learning_rates:
                        final_lr = learning_rates[-1]
                        print(f"  {'æœ€ç»ˆå­¦ä¹ ç‡':<25}: {final_lr:.2e}")
                        
        except Exception as e:
            logger.warning(f"Failed to extract training statistics: {e}")
        
        # 4. è¯„ä¼°ç»Ÿè®¡
        print("\nğŸ“ˆ è¯„ä¼°ç»Ÿè®¡:")
        print("-" * 40)
        try:
            if 'eval_runtime' in eval_results:
                eval_runtime = eval_results['eval_runtime']
                print(f"  {'è¯„ä¼°æ—¶é—´':<25}: {eval_runtime:.2f} ç§’")
            
            if 'eval_samples_per_second' in eval_results:
                eval_speed = eval_results['eval_samples_per_second']
                print(f"  {'è¯„ä¼°é€Ÿåº¦':<25}: {eval_speed:.2f} æ ·æœ¬/ç§’")
                
        except Exception as e:
            logger.warning(f"Failed to extract evaluation statistics: {e}")
        
        # 5. ç¡¬ä»¶ä½¿ç”¨æƒ…å†µ
        print("\nğŸ’» ç¡¬ä»¶ä½¿ç”¨:")
        print("-" * 40)
        try:
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                print(f"  {'GPUæ•°é‡':<25}: {gpu_count}")
                
                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                for i in range(gpu_count):
                    memory_allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
                    memory_reserved = torch.cuda.memory_reserved(i) / 1024**3   # GB
                    print(f"  {'GPU ' + str(i) + ' å†…å­˜ä½¿ç”¨':<25}: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")
            else:
                print(f"  {'è®¾å¤‡':<25}: CPU")
                
        except Exception as e:
            logger.warning(f"Failed to get hardware information: {e}")
        
        # 6. é…ç½®ä¿¡æ¯
        print("\nâš™ï¸ å®éªŒé…ç½®:")
        print("-" * 40)
        print(f"  {'æ¨¡å‹':<25}: {self.config.model_name}")
        print(f"  {'ä»»åŠ¡':<25}: {task}")
        print(f"  {'æ–¹æ³•':<25}: {method}")
        print(f"  {'æ‰¹æ¬¡å¤§å°':<25}: {self.config.batch_size}")
        print(f"  {'å­¦ä¹ ç‡':<25}: {self.config.learning_rate}")
        print(f"  {'è®­ç»ƒè½®æ•°':<25}: {self.config.num_epochs}")
        print(f"  {'æœ€å¤§åºåˆ—é•¿åº¦':<25}: {self.config.max_seq_length}")
        
        print("\n" + "="*80)
        print(f"å®éªŒ {method.upper()} on {task.upper()} å®Œæˆ!")
        print("="*80 + "\n")
    
    def run_all_experiments(self, methods: List[str]) -> Dict[str, Dict[str, float]]:
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        all_results = {}
        visualization_data = []
        
        for method in methods:
            method_results = {}
            for task in self.config.tasks:
                try:
                    results = self.run_experiment(method, task)
                    method_results[task] = results
                    logger.info(f"{method} on {task}: {results}")
                    
                    # æ”¶é›†å¯è§†åŒ–æ•°æ®
                    if self.visualizer and 'eval_accuracy' in results:
                        viz_data = {
                            'method': method,
                            'task': task,
                            'benchmarks': {
                                'performance': {
                                    'accuracy': results['eval_accuracy'],
                                    'throughput': results.get('eval_samples_per_second', 0),
                                    'avg_latency': results.get('eval_runtime', 0) * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                                    'memory_peak_mb': results.get('train_memory_peak_mb', 0)
                                }
                            }
                        }
                        visualization_data.append(viz_data)
                        
                except Exception as e:
                    logger.error(f"Error running {method} on {task}: {e}")
                    method_results[task] = {"error": str(e)}
            
            all_results[method] = method_results
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        with open(f"{self.config.output_dir}/all_results.json", "w") as f:
            json.dump(all_results, f, indent=2)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        if self.visualizer and visualization_data:
            try:
                logger.info("Generating visualization charts...")
                
                # æ€§èƒ½å¯¹æ¯”å›¾è¡¨
                perf_chart_path = self.visualizer.visualize_performance_comparison(
                    visualization_data, 
                    save_path=Path(self.config.output_dir) / "performance_comparison.png"
                )
                logger.info(f"Performance comparison chart saved to: {perf_chart_path}")
                
                # åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿ï¼ˆå¦‚æœæ”¯æŒplotlyï¼‰
                try:
                    dashboard_path = self.visualizer.create_interactive_dashboard(
                        visualization_data,
                        save_path=Path(self.config.output_dir) / "interactive_dashboard.html"
                    )
                    logger.info(f"Interactive dashboard saved to: {dashboard_path}")
                except Exception as e:
                    logger.warning(f"Failed to create interactive dashboard: {e}")
                    
            except Exception as e:
                logger.warning(f"Failed to generate visualization charts: {e}")
        
        return all_results
    
    def generate_comparison_report(self, results: Dict[str, Dict[str, float]]):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        report = []
        report.append("# GLUE Benchmark Results\n")
        
        # åˆ›å»ºç»“æœè¡¨æ ¼
        methods = list(results.keys())
        tasks = self.config.tasks
        
        # è¡¨å¤´
        header = "| Method | " + " | ".join(tasks) + " | Average |\n"
        separator = "|" + "|".join(["---"] * (len(tasks) + 2)) + "|\n"
        report.append(header)
        report.append(separator)
        
        # æ¯ä¸ªæ–¹æ³•çš„ç»“æœ
        for method in methods:
            row = f"| {method} |"
            scores = []
            
            for task in tasks:
                if task in results[method] and "error" not in results[method][task]:
                    task_config = self.data_processor.TASK_CONFIGS[task]
                    metric_key = f"eval_{task_config['metric']}"
                    if metric_key in results[method][task]:
                        score = results[method][task][metric_key]
                        scores.append(score)
                        row += f" {score:.4f} |"
                    else:
                        row += " N/A |"
                        scores.append(0.0)
                else:
                    row += " Error |"
                    scores.append(0.0)
            
            # è®¡ç®—å¹³å‡åˆ†
            avg_score = np.mean(scores) if scores else 0.0
            row += f" {avg_score:.4f} |\n"
            report.append(row)
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f"{self.config.output_dir}/comparison_report.md", "w") as f:
            f.writelines(report)
        
        logger.info("Comparison report saved to comparison_report.md")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='GLUEåŸºå‡†æµ‹è¯•ï¼šLoRAã€AdaLoRAã€DoRAã€LoRAvenå¯¹æ¯”å®éªŒ')
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('--model_name', type=str, default='bert-base-uncased',
                        help='é¢„è®­ç»ƒæ¨¡å‹åç§° (é»˜è®¤: bert-base-uncased)')
    parser.add_argument('--tasks', type=str, nargs='+', default=["sst2", "mnli", "qnli", "rte", "cola"],
                        help='è¦è¿è¡Œçš„GLUEä»»åŠ¡åˆ—è¡¨ (é»˜è®¤: æ‰€æœ‰ä»»åŠ¡)')
    parser.add_argument('--methods', type=str, nargs='+', default=["lora"],
                        help='è¦æµ‹è¯•çš„æ–¹æ³•åˆ—è¡¨ (å¯é€‰: lora, adalora, dora, loraven, full_finetune) (é»˜è®¤: lora)')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--num_epochs', '--epochs', type=int, default=5,
                        help='è®­ç»ƒè½®æ•° (é»˜è®¤: 5)')
    parser.add_argument('--warmup_epochs', type=float, default=1,
                        help='é¢„çƒ­è½®æ•°ï¼š>=1è¡¨ç¤ºå…·ä½“è½®æ•°ï¼Œ<1è¡¨ç¤ºæ€»è½®æ•°çš„æ¯”ä¾‹ (é»˜è®¤: 1)')
    parser.add_argument('--optimizer', type=str, default='adamw_hf',
                        choices=['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adafactor', 'sgd', 'adagrad', 'rmsprop'],
                        help='ä¼˜åŒ–å™¨ç±»å‹ (é»˜è®¤: adamw_hf)')
    parser.add_argument('--lr', '--learning_rate', type=float, default=2e-4,
                        help='å­¦ä¹ ç‡ (é»˜è®¤: 2e-4)')
    parser.add_argument('--batch_size', type=int, default=64,
                        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 64)')
    parser.add_argument('--weight_decay', type=float, default=0.01,
                        help='æƒé‡è¡°å‡ (é»˜è®¤: 0.01)')
    parser.add_argument('--lr_scheduler', type=str, default='linear',
                        choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'],
                        help='å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ (é»˜è®¤: linear)')
    parser.add_argument('--max_seq_length', type=int, default=128,
                        help='æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 128)')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­ (é»˜è®¤: 42)')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, default='./glue_results',
                        help='è¾“å‡ºç›®å½• (é»˜è®¤: ./glue_results)')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--use_mixed_precision', action='store_true', default=True,
                        help='æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (é»˜è®¤: True)')
    parser.add_argument('--dataloader_num_workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 4)')
    
    # å¯è§†åŒ–å‚æ•°
    parser.add_argument('--enable_visualization', action='store_true', default=True,
                        help='æ˜¯å¦å¯ç”¨å¯è§†åŒ–åŠŸèƒ½ (é»˜è®¤: True)')
    parser.add_argument('--disable_visualization', action='store_true', default=False,
                        help='ç¦ç”¨å¯è§†åŒ–åŠŸèƒ½')

    # YAMLé…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str, default=os.path.join(os.path.dirname(__file__), 'glue_config.yaml'),
                        help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: experiments/glue_config.yaml)')
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    # åŠ è½½YAMLé…ç½®
    yaml_overrides = {}
    if args.config and os.path.exists(args.config):
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                yaml_overrides = yaml.safe_load(f) or {}
            logger.info(f"åŠ è½½YAMLé…ç½®: {args.config}")
        except Exception as e:
            logger.warning(f"YAMLé…ç½®åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨CLIä¸é»˜è®¤å‚æ•°: {e}")
    else:
        logger.info("æœªæä¾›æˆ–æ‰¾ä¸åˆ°YAMLé…ç½®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨CLIä¸é»˜è®¤å‚æ•°")
    
    # åˆ›å»ºå®éªŒé…ç½®
    config = ExperimentConfig(
        model_name=args.model_name,
        tasks=args.tasks,
        max_seq_length=args.max_seq_length,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        num_epochs=args.num_epochs,
        warmup_epochs=args.warmup_epochs,
        optimizer=args.optimizer,
        weight_decay=args.weight_decay,
        lr_scheduler=args.lr_scheduler,
        output_dir=args.output_dir,
        seed=args.seed,
        use_mixed_precision=args.use_mixed_precision,
        dataloader_num_workers=args.dataloader_num_workers,
        enable_visualization=args.enable_visualization and not args.disable_visualization
    )
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    logger.info("=== å®éªŒé…ç½® ===")
    logger.info(f"æ¨¡å‹: {config.model_name}")
    logger.info(f"ä»»åŠ¡: {config.tasks}")
    logger.info(f"æ–¹æ³•: {args.methods}")
    logger.info(f"è®­ç»ƒè½®æ•°: {config.num_epochs}")
    logger.info(f"é¢„çƒ­è½®æ•°: {config.warmup_epochs}")
    logger.info(f"ä¼˜åŒ–å™¨: {config.optimizer}")
    logger.info(f"å­¦ä¹ ç‡: {config.learning_rate}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    logger.info(f"æƒé‡è¡°å‡: {config.weight_decay}")
    logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {config.lr_scheduler}")
    logger.info(f"æœ€å¤§åºåˆ—é•¿åº¦: {config.max_seq_length}")
    logger.info(f"éšæœºç§å­: {config.seed}")
    logger.info(f"è¾“å‡ºç›®å½•: {config.output_dir}")
    logger.info(f"å¯è§†åŒ–åŠŸèƒ½: {'å¯ç”¨' if config.enable_visualization else 'ç¦ç”¨'}")
    logger.info("================")
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•
    benchmark = GLUEBenchmark(config, yaml_overrides=yaml_overrides)
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results = benchmark.run_all_experiments(args.methods)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    benchmark.generate_comparison_report(results)
    
    logger.info("=== å®éªŒå®Œæˆ ===")
    if config.enable_visualization and benchmark.visualizer:
        logger.info("å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°è¾“å‡ºç›®å½•ä¸­")
        logger.info(f"- æ€§èƒ½å¯¹æ¯”å›¾è¡¨: {config.output_dir}/performance_comparison.png")
        logger.info(f"- äº¤äº’å¼ä»ªè¡¨æ¿: {config.output_dir}/interactive_dashboard.html")
    logger.info(f"è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: {config.output_dir}/all_results.json")
    logger.info("================")
    
    print("GLUE benchmark completed!")
    print(f"Results saved to {config.output_dir}")

if __name__ == "__main__":
    main()